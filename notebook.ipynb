{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "572579d1",
   "metadata": {},
   "source": [
    "# Speech-to-Text Benchmarking using Word Error Rate (WER)\n",
    "\n",
    "This notebook benchmarks three leading Speech-to-Text engines:\n",
    "1. OpenAI Whisper  \n",
    "2. faster-whisper  \n",
    "3. Vosk  \n",
    "\n",
    "We evaluate them using **Word Error Rate (WER)** on a small test set of audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a2f46",
   "metadata": {},
   "source": [
    "## What is Word Error Rate (WER)?\n",
    "\n",
    "WER is a standard metric for evaluating speech recognition systems.\n",
    "\n",
    "WER = (Substitutions + Deletions + Insertions) / Number of words in reference\n",
    "\n",
    "Lower WER = better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e10dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai-whisper faster-whisper vosk jiwer soundfile librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f1975",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We use 5 short audio samples (25 - 40 sec) with known ground-truth transcripts.\n",
    "Each audio file has a corresponding `.txt` file containing the correct transcription.\n",
    "\n",
    "Directory structure:\n",
    "\n",
    "/dataset\n",
    "1.     /audio-1.wav\n",
    "2.     /audio-2.wav\n",
    "3.     /audio-3.wav\n",
    "4.     /audio-4.wav\n",
    "5.     /audio-5.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3e73a",
   "metadata": {},
   "source": [
    "## Cell 2: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12889396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import whisper\n",
    "from faster_whisper import WhisperModel\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import json\n",
    "import wave\n",
    "import soundfile as sf\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa9b582",
   "metadata": {},
   "source": [
    "## Cell 3: Loading Test Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/dataset\"\n",
    "\n",
    "def load_test_data(dataset_path):\n",
    "    data = []\n",
    "    for file in os.listdir(dataset_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            audio_path = os.path.join(dataset_path, file)\n",
    "            txt_path = audio_path.replace(\".wav\", \".txt\")\n",
    "            if os.path.exists(txt_path):\n",
    "                with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    reference = f.read().strip()\n",
    "                data.append((audio_path, reference))\n",
    "    return data\n",
    "\n",
    "test_data = load_test_data(DATASET_PATH)\n",
    "\n",
    "print(f\"Loaded {len(test_data)} audio samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef32b9",
   "metadata": {},
   "source": [
    "## Model 1: OpenAI Whisper\n",
    "High accuracy multilingual transformer-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2579b10",
   "metadata": {},
   "source": [
    "## Cell 4: Whisper Transcription + WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b7509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_whisper(model, test_data):\n",
    "    wers = []\n",
    "    for audio_path, reference in test_data:\n",
    "        result = model.transcribe(audio_path)\n",
    "        prediction = result[\"text\"].strip()\n",
    "        error = wer(reference.lower(), prediction.lower())\n",
    "        wers.append(error)\n",
    "        print(f\"Whisper WER for {os.path.basename(audio_path)}: {error}\")\n",
    "    return sum(wers) / len(wers)\n",
    "\n",
    "whisper_avg_wer = evaluate_whisper(whisper_model, test_data)\n",
    "print(f\"\\nAverage Whisper WER: {whisper_avg_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd63cfa",
   "metadata": {},
   "source": [
    "## Model 2: faster-whisper\n",
    "Optimized implementation of Whisper using CTranslate2 for faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28dc414",
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_model = WhisperModel(\"base\", device=\"cpu\", compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c5697",
   "metadata": {},
   "source": [
    "## Cell 5: Faster-whisper Transcription + WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b04a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faster_whisper(model, test_data):\n",
    "    wers = []\n",
    "    for audio_path, reference in test_data:\n",
    "        segments, info = model.transcribe(audio_path)\n",
    "        prediction = \" \".join([segment.text for segment in segments]).strip()\n",
    "        error = wer(reference.lower(), prediction.lower())\n",
    "        wers.append(error)\n",
    "        print(f\"faster-whisper WER for {os.path.basename(audio_path)}: {error}\")\n",
    "    return sum(wers) / len(wers)\n",
    "\n",
    "faster_whisper_avg_wer = evaluate_faster_whisper(faster_model, test_data)\n",
    "print(f\"\\nAverage faster-whisper WER: {faster_whisper_avg_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02988001",
   "metadata": {},
   "source": [
    "## Model 3: Vosk\n",
    "Lightweight offline speech recognition engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de41e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
    "!unzip vosk-model-small-en-us-0.15.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be0890",
   "metadata": {},
   "outputs": [],
   "source": [
    "vosk_model = Model(\"vosk-model-small-en-us-0.15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d4b70",
   "metadata": {},
   "source": [
    "## Cell 6:  Vosk Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_vosk(audio_path, model):\n",
    "    wf = wave.open(audio_path, \"rb\")\n",
    "    rec = KaldiRecognizer(model, wf.getframerate())\n",
    "    rec.SetWords(True)\n",
    "\n",
    "    result_text = \"\"\n",
    "\n",
    "    while True:\n",
    "        data = wf.readframes(4000)\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "        if rec.AcceptWaveform(data):\n",
    "            res = json.loads(rec.Result())\n",
    "            result_text += \" \" + res.get(\"text\", \"\")\n",
    "\n",
    "    final_res = json.loads(rec.FinalResult())\n",
    "    result_text += \" \" + final_res.get(\"text\", \"\")\n",
    "\n",
    "    return result_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafba3b",
   "metadata": {},
   "source": [
    "## Cell 7: Vosk Transcription + WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vosk(model, test_data):\n",
    "    wers = []\n",
    "    for audio_path, reference in test_data:\n",
    "        prediction = transcribe_vosk(audio_path, model)\n",
    "        error = wer(reference.lower(), prediction.lower())\n",
    "        wers.append(error)\n",
    "        print(f\"Vosk WER for {os.path.basename(audio_path)}: {error}\")\n",
    "    return sum(wers) / len(wers)\n",
    "\n",
    "vosk_avg_wer = evaluate_vosk(vosk_model, test_data)\n",
    "print(f\"\\nAverage Vosk WER: {vosk_avg_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc717f68",
   "metadata": {},
   "source": [
    "## Final WER Comparison\n",
    "Lower is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = {\n",
    "    \"Model\": [\"Whisper\", \"faster-whisper\", \"Vosk\"],\n",
    "    \"Average WER\": [whisper_avg_wer, faster_whisper_avg_wer, vosk_avg_wer]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
