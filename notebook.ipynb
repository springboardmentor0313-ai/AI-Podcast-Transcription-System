{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "572579d1",
   "metadata": {},
   "source": [
    "# Task 1: Speech-to-Text Benchmarking using Word Error Rate (WER)\n",
    "\n",
    "This notebook benchmarks three leading Speech-to-Text engines:\n",
    "1. OpenAI Whisper  \n",
    "2. faster-whisper  \n",
    "3. Vosk  \n",
    "\n",
    "We evaluate them using **Word Error Rate (WER)** on a small test set of audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a2f46",
   "metadata": {},
   "source": [
    "## What is Word Error Rate (WER)?\n",
    "\n",
    "WER is a standard metric for evaluating speech recognition systems.\n",
    "\n",
    "WER = (Substitutions + Deletions + Insertions) / Number of words in reference\n",
    "\n",
    "Lower WER = better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e10dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai-whisper faster-whisper vosk jiwer soundfile librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f1975",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We use 5 short audio samples (25 - 40 sec) with known ground-truth transcripts.\n",
    "Each audio file has a corresponding `.txt` file containing the correct transcription.\n",
    "\n",
    "Directory structure:\n",
    "\n",
    "/dataset\n",
    "1.     /audio-1.wav\n",
    "2.     /audio-2.wav\n",
    "3.     /audio-3.wav\n",
    "4.     /audio-4.wav\n",
    "5.     /audio-5.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3e73a",
   "metadata": {},
   "source": [
    "## Cell 2: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12889396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import whisper\n",
    "from faster_whisper import WhisperModel\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import json\n",
    "import wave\n",
    "import soundfile as sf\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa9b582",
   "metadata": {},
   "source": [
    "## Cell 3: Loading Test Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/dataset\"\n",
    "\n",
    "def load_test_data(dataset_path):\n",
    "    data = []\n",
    "    for file in os.listdir(dataset_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            audio_path = os.path.join(dataset_path, file)\n",
    "            txt_path = audio_path.replace(\".wav\", \".txt\")\n",
    "            if os.path.exists(txt_path):\n",
    "                with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    reference = f.read().strip()\n",
    "                data.append((audio_path, reference))\n",
    "    return data\n",
    "\n",
    "test_data = load_test_data(DATASET_PATH)\n",
    "\n",
    "print(f\"Loaded {len(test_data)} audio samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef32b9",
   "metadata": {},
   "source": [
    "## Model 1: OpenAI Whisper\n",
    "High accuracy multilingual transformer-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2579b10",
   "metadata": {},
   "source": [
    "## Cell 4: Whisper Transcription + WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b7509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_whisper(model, test_data):\n",
    "    wers = []\n",
    "    for audio_path, reference in test_data:\n",
    "        result = model.transcribe(audio_path)\n",
    "        prediction = result[\"text\"].strip()\n",
    "        error = wer(reference.lower(), prediction.lower())\n",
    "        wers.append(error)\n",
    "        print(f\"Whisper WER for {os.path.basename(audio_path)}: {error}\")\n",
    "    return sum(wers) / len(wers)\n",
    "\n",
    "whisper_avg_wer = evaluate_whisper(whisper_model, test_data)\n",
    "print(f\"\\nAverage Whisper WER: {whisper_avg_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd63cfa",
   "metadata": {},
   "source": [
    "## Model 2: faster-whisper\n",
    "Optimized implementation of Whisper using CTranslate2 for faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28dc414",
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_model = WhisperModel(\"base\", device=\"cpu\", compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c5697",
   "metadata": {},
   "source": [
    "## Cell 5: Faster-whisper Transcription + WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b04a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faster_whisper(model, test_data):\n",
    "    wers = []\n",
    "    for audio_path, reference in test_data:\n",
    "        segments, info = model.transcribe(audio_path)\n",
    "        prediction = \" \".join([segment.text for segment in segments]).strip()\n",
    "        error = wer(reference.lower(), prediction.lower())\n",
    "        wers.append(error)\n",
    "        print(f\"faster-whisper WER for {os.path.basename(audio_path)}: {error}\")\n",
    "    return sum(wers) / len(wers)\n",
    "\n",
    "faster_whisper_avg_wer = evaluate_faster_whisper(faster_model, test_data)\n",
    "print(f\"\\nAverage faster-whisper WER: {faster_whisper_avg_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02988001",
   "metadata": {},
   "source": [
    "## Model 3: Vosk\n",
    "Lightweight offline speech recognition engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de41e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
    "!unzip vosk-model-small-en-us-0.15.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be0890",
   "metadata": {},
   "outputs": [],
   "source": [
    "vosk_model = Model(\"vosk-model-small-en-us-0.15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d4b70",
   "metadata": {},
   "source": [
    "## Cell 6:  Vosk Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_vosk(audio_path, model):\n",
    "    wf = wave.open(audio_path, \"rb\")\n",
    "    rec = KaldiRecognizer(model, wf.getframerate())\n",
    "    rec.SetWords(True)\n",
    "\n",
    "    result_text = \"\"\n",
    "\n",
    "    while True:\n",
    "        data = wf.readframes(4000)\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "        if rec.AcceptWaveform(data):\n",
    "            res = json.loads(rec.Result())\n",
    "            result_text += \" \" + res.get(\"text\", \"\")\n",
    "\n",
    "    final_res = json.loads(rec.FinalResult())\n",
    "    result_text += \" \" + final_res.get(\"text\", \"\")\n",
    "\n",
    "    return result_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafba3b",
   "metadata": {},
   "source": [
    "## Cell 7: Vosk Transcription + WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vosk(model, test_data):\n",
    "    wers = []\n",
    "    for audio_path, reference in test_data:\n",
    "        prediction = transcribe_vosk(audio_path, model)\n",
    "        error = wer(reference.lower(), prediction.lower())\n",
    "        wers.append(error)\n",
    "        print(f\"Vosk WER for {os.path.basename(audio_path)}: {error}\")\n",
    "    return sum(wers) / len(wers)\n",
    "\n",
    "vosk_avg_wer = evaluate_vosk(vosk_model, test_data)\n",
    "print(f\"\\nAverage Vosk WER: {vosk_avg_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc717f68",
   "metadata": {},
   "source": [
    "## Final WER Comparison\n",
    "Lower is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = {\n",
    "    \"Model\": [\"Whisper\", \"faster-whisper\", \"Vosk\"],\n",
    "    \"Average WER\": [whisper_avg_wer, faster_whisper_avg_wer, vosk_avg_wer]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a271779",
   "metadata": {},
   "source": [
    "# Task 2: Transcription using faster-whisper "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288b1cb",
   "metadata": {},
   "source": [
    "### Cell 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff606725",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faster-whisper soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c58a00c",
   "metadata": {},
   "source": [
    "### Cell 2: Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f97c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import math\n",
    "from faster_whisper import WhisperModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb5996",
   "metadata": {},
   "source": [
    "### Cell 3: Loading Podcast Audio File \n",
    "\n",
    "We here load the mp3 audio file and convert it to proper mono 16k Hz wav format, for proper transcribing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55901968",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -i /podcast-audio/807931c237e75122fd4f0bb4ec9f7d1b.mp3 -ac 1 -ar 16000 clean_audio.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a524c",
   "metadata": {},
   "source": [
    "### Cell 4: Loading the wav file, and checking it's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fd3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "audio, sr = sf.read(\"clean_audio.wav\")\n",
    "print(\"Sample rate:\", sr)\n",
    "print(\"Shape:\", audio.shape)\n",
    "print(\"Duration (sec):\", len(audio) / sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73375a",
   "metadata": {},
   "source": [
    "### Cell 5: Splitting Audio into 45-Second Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import soundfile as sf\n",
    "\n",
    "def split_audio_correct(audio_path, chunk_duration=45):\n",
    "    audio, sr = sf.read(audio_path)\n",
    "\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = audio.mean(axis=1)  # force mono\n",
    "\n",
    "    total_samples = len(audio)\n",
    "    samples_per_chunk = int(chunk_duration * sr)\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    for start in range(0, total_samples, samples_per_chunk):\n",
    "        end = start + samples_per_chunk\n",
    "        chunk = audio[start:end]\n",
    "\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "\n",
    "        chunks.append(chunk.astype(np.float32))\n",
    "\n",
    "    print(f\"Total chunks created: {len(chunks)}\")\n",
    "    return chunks, sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9196c604",
   "metadata": {},
   "source": [
    "### Cell 6: Loading faster-whisper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c984915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model = WhisperModel(\n",
    "    \"base\",\n",
    "    device=\"cpu\",\n",
    "    compute_type=\"int8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce71d8",
   "metadata": {},
   "source": [
    "### Cell 7: Timeline formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d8e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timestamp(seconds):\n",
    "    hrs = int(seconds // 3600)\n",
    "    mins = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    return f\"{hrs:02d}:{mins:02d}:{secs:02d}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa823f6",
   "metadata": {},
   "source": [
    "### Cell 8: Transcribing Long Audio using Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1030655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_long_audio_with_timestamps(model, audio_path, chunk_duration=45):\n",
    "    chunks, sr = split_audio_correct(audio_path, chunk_duration)\n",
    "    full_text = []\n",
    "\n",
    "    print(\"\\n================= STARTING TRANSCRIPTION =================\\n\")\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        start_time = idx * chunk_duration\n",
    "        end_time = start_time + (len(chunk) / sr)\n",
    "\n",
    "        start_ts = format_timestamp(start_time)\n",
    "        end_ts = format_timestamp(end_time)\n",
    "\n",
    "        print(f\"\\n[{start_ts} - {end_ts}]\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        segments, info = model.transcribe(\n",
    "            chunk,\n",
    "            language=\"en\",\n",
    "            beam_size=5,\n",
    "            temperature=0.0,\n",
    "            vad_filter=True,\n",
    "            vad_parameters=dict(min_silence_duration_ms=500)\n",
    "        )\n",
    "\n",
    "        chunk_text = \"\"\n",
    "        for segment in segments:\n",
    "            chunk_text += segment.text + \" \"\n",
    "\n",
    "        chunk_text = chunk_text.strip()\n",
    "\n",
    "        # Print chunk transcription\n",
    "        print(chunk_text)\n",
    "\n",
    "        # Store\n",
    "        full_text.append(chunk_text)\n",
    "\n",
    "    print(\"\\n================= TRANSCRIPTION COMPLETE =================\\n\")\n",
    "\n",
    "    final_transcript = \" \".join(full_text)\n",
    "\n",
    "    print(\"\\n============= FULL TRANSCRIPT (COMBINED) =============\\n\")\n",
    "    print(final_transcript)\n",
    "\n",
    "    return full_text, final_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ae2f7",
   "metadata": {},
   "source": [
    "### Cell 9: Running Transcription on Podcast Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_texts, final_transcript = transcribe_long_audio_with_timestamps(\n",
    "    model,\n",
    "    \"clean_audio.wav\",\n",
    "    chunk_duration=45\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604650b",
   "metadata": {},
   "source": [
    "### Cell 8: Saving Final Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff45985",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"/output/podcast_transcript_with_timestamps.txt\"\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, text in enumerate(chunk_texts):\n",
    "        start_time = idx * 45\n",
    "        end_time = start_time + len(text)\n",
    "\n",
    "        start_ts = format_timestamp(start_time)\n",
    "        end_ts = format_timestamp(start_time + 45)\n",
    "\n",
    "        f.write(f\"[{start_ts} - {end_ts}]\\n\")\n",
    "        f.write(text + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"\\n================ FULL TRANSCRIPT ================\\n\\n\")\n",
    "    f.write(final_transcript)\n",
    "\n",
    "print(\"âœ… Transcript saved to:\", OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
