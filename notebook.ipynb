{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "572579d1",
   "metadata": {},
   "source": [
    "# Task 1: Speech-to-Text Benchmarking using Word Error Rate (WER)\n",
    "\n",
    "This notebook benchmarks three leading Speech-to-Text engines:\n",
    "1. OpenAI Whisper  \n",
    "2. faster-whisper  \n",
    "3. Vosk  \n",
    "\n",
    "We evaluate them using **Word Error Rate (WER)** on a small test set of audio files.\n",
    "\n",
    "Report Link: https://drive.google.com/file/d/1PVfo9WylX4lqOrTO2MT0cnDdks1DNGbn/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a2f46",
   "metadata": {},
   "source": [
    "## What is Word Error Rate (WER)?\n",
    "\n",
    "WER is a standard metric for evaluating speech recognition systems.\n",
    "\n",
    "WER = (Substitutions + Deletions + Insertions) / Number of words in reference\n",
    "\n",
    "Lower WER = better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e10dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai-whisper faster-whisper vosk jiwer soundfile librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f1975",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We use 5 short audio samples (25 - 40 sec) with known ground-truth transcripts.\n",
    "Each audio file has a corresponding `.txt` file containing the correct transcription.\n",
    "\n",
    "Directory structure:\n",
    "\n",
    "/dataset\n",
    "1.     /audio-1.wav\n",
    "2.     /audio-2.wav\n",
    "3.     /audio-3.wav\n",
    "4.     /audio-4.wav\n",
    "5.     /audio-5.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3e73a",
   "metadata": {},
   "source": [
    "## Cell 2: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12889396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import whisper\n",
    "from faster_whisper import WhisperModel\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import json\n",
    "import wave\n",
    "import soundfile as sf\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa9b582",
   "metadata": {},
   "source": [
    "## Cell 3: Loading Test Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/dataset\"\n",
    "\n",
    "def load_test_data(dataset_path):\n",
    "    data = []\n",
    "    for file in os.listdir(dataset_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            audio_path = os.path.join(dataset_path, file)\n",
    "            txt_path = audio_path.replace(\".wav\", \".txt\")\n",
    "            if os.path.exists(txt_path):\n",
    "                with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    reference = f.read().strip()\n",
    "                data.append((audio_path, reference))\n",
    "    return data\n",
    "\n",
    "test_data = load_test_data(DATASET_PATH)\n",
    "\n",
    "print(f\"Loaded {len(test_data)} audio samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef32b9",
   "metadata": {},
   "source": [
    "## Model 1: OpenAI Whisper\n",
    "High accuracy multilingual transformer-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2579b10",
   "metadata": {},
   "source": [
    "## Cell 4: Whisper Transcription + WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b7509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_whisper(model, test_data):\n",
    "    wers = []\n",
    "    for audio_path, reference in test_data:\n",
    "        result = model.transcribe(audio_path)\n",
    "        prediction = result[\"text\"].strip()\n",
    "        error = wer(reference.lower(), prediction.lower())\n",
    "        wers.append(error)\n",
    "        print(f\"Whisper WER for {os.path.basename(audio_path)}: {error}\")\n",
    "    return sum(wers) / len(wers)\n",
    "\n",
    "whisper_avg_wer = evaluate_whisper(whisper_model, test_data)\n",
    "print(f\"\\nAverage Whisper WER: {whisper_avg_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd63cfa",
   "metadata": {},
   "source": [
    "## Model 2: faster-whisper\n",
    "Optimized implementation of Whisper using CTranslate2 for faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28dc414",
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_model = WhisperModel(\"base\", device=\"cpu\", compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c5697",
   "metadata": {},
   "source": [
    "## Cell 5: Faster-whisper Transcription + WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b04a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faster_whisper(model, test_data):\n",
    "    wers = []\n",
    "    for audio_path, reference in test_data:\n",
    "        segments, info = model.transcribe(audio_path)\n",
    "        prediction = \" \".join([segment.text for segment in segments]).strip()\n",
    "        error = wer(reference.lower(), prediction.lower())\n",
    "        wers.append(error)\n",
    "        print(f\"faster-whisper WER for {os.path.basename(audio_path)}: {error}\")\n",
    "    return sum(wers) / len(wers)\n",
    "\n",
    "faster_whisper_avg_wer = evaluate_faster_whisper(faster_model, test_data)\n",
    "print(f\"\\nAverage faster-whisper WER: {faster_whisper_avg_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02988001",
   "metadata": {},
   "source": [
    "## Model 3: Vosk\n",
    "Lightweight offline speech recognition engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de41e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
    "!unzip vosk-model-small-en-us-0.15.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be0890",
   "metadata": {},
   "outputs": [],
   "source": [
    "vosk_model = Model(\"vosk-model-small-en-us-0.15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d4b70",
   "metadata": {},
   "source": [
    "## Cell 6:  Vosk Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_vosk(audio_path, model):\n",
    "    wf = wave.open(audio_path, \"rb\")\n",
    "    rec = KaldiRecognizer(model, wf.getframerate())\n",
    "    rec.SetWords(True)\n",
    "\n",
    "    result_text = \"\"\n",
    "\n",
    "    while True:\n",
    "        data = wf.readframes(4000)\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "        if rec.AcceptWaveform(data):\n",
    "            res = json.loads(rec.Result())\n",
    "            result_text += \" \" + res.get(\"text\", \"\")\n",
    "\n",
    "    final_res = json.loads(rec.FinalResult())\n",
    "    result_text += \" \" + final_res.get(\"text\", \"\")\n",
    "\n",
    "    return result_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafba3b",
   "metadata": {},
   "source": [
    "## Cell 7: Vosk Transcription + WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vosk(model, test_data):\n",
    "    wers = []\n",
    "    for audio_path, reference in test_data:\n",
    "        prediction = transcribe_vosk(audio_path, model)\n",
    "        error = wer(reference.lower(), prediction.lower())\n",
    "        wers.append(error)\n",
    "        print(f\"Vosk WER for {os.path.basename(audio_path)}: {error}\")\n",
    "    return sum(wers) / len(wers)\n",
    "\n",
    "vosk_avg_wer = evaluate_vosk(vosk_model, test_data)\n",
    "print(f\"\\nAverage Vosk WER: {vosk_avg_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc717f68",
   "metadata": {},
   "source": [
    "## Final WER Comparison\n",
    "Lower is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = {\n",
    "    \"Model\": [\"Whisper\", \"faster-whisper\", \"Vosk\"],\n",
    "    \"Average WER\": [whisper_avg_wer, faster_whisper_avg_wer, vosk_avg_wer]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a271779",
   "metadata": {},
   "source": [
    "# Task 2: Transcription using faster-whisper \n",
    "\n",
    "Report LinK: https://drive.google.com/file/d/1PVfo9WylX4lqOrTO2MT0cnDdks1DNGbn/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288b1cb",
   "metadata": {},
   "source": [
    "### Cell 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff606725",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faster-whisper soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c58a00c",
   "metadata": {},
   "source": [
    "### Cell 2: Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f97c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import math\n",
    "from faster_whisper import WhisperModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb5996",
   "metadata": {},
   "source": [
    "### Cell 3: Loading Podcast Audio File \n",
    "\n",
    "We here load the mp3 audio file and convert it to proper mono 16k Hz wav format, for proper transcribing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55901968",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -i /podcast-audio/807931c237e75122fd4f0bb4ec9f7d1b.mp3 -ac 1 -ar 16000 clean_audio.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a524c",
   "metadata": {},
   "source": [
    "### Cell 4: Loading the wav file, and checking it's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fd3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "audio, sr = sf.read(\"clean_audio.wav\")\n",
    "print(\"Sample rate:\", sr)\n",
    "print(\"Shape:\", audio.shape)\n",
    "print(\"Duration (sec):\", len(audio) / sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73375a",
   "metadata": {},
   "source": [
    "### Cell 5: Splitting Audio into 45-Second Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import soundfile as sf\n",
    "\n",
    "def split_audio_correct(audio_path, chunk_duration=45):\n",
    "    audio, sr = sf.read(audio_path)\n",
    "\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = audio.mean(axis=1)  # force mono\n",
    "\n",
    "    total_samples = len(audio)\n",
    "    samples_per_chunk = int(chunk_duration * sr)\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    for start in range(0, total_samples, samples_per_chunk):\n",
    "        end = start + samples_per_chunk\n",
    "        chunk = audio[start:end]\n",
    "\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "\n",
    "        chunks.append(chunk.astype(np.float32))\n",
    "\n",
    "    print(f\"Total chunks created: {len(chunks)}\")\n",
    "    return chunks, sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9196c604",
   "metadata": {},
   "source": [
    "### Cell 6: Loading faster-whisper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c984915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model = WhisperModel(\n",
    "    \"base\",\n",
    "    device=\"cpu\",\n",
    "    compute_type=\"int8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce71d8",
   "metadata": {},
   "source": [
    "### Cell 7: Timeline formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d8e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timestamp(seconds):\n",
    "    hrs = int(seconds // 3600)\n",
    "    mins = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    return f\"{hrs:02d}:{mins:02d}:{secs:02d}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa823f6",
   "metadata": {},
   "source": [
    "### Cell 8: Transcribing Long Audio using Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1030655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_long_audio_with_timestamps(model, audio_path, chunk_duration=45):\n",
    "    chunks, sr = split_audio_correct(audio_path, chunk_duration)\n",
    "    full_text = []\n",
    "\n",
    "    print(\"\\n================= STARTING TRANSCRIPTION =================\\n\")\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        start_time = idx * chunk_duration\n",
    "        end_time = start_time + (len(chunk) / sr)\n",
    "\n",
    "        start_ts = format_timestamp(start_time)\n",
    "        end_ts = format_timestamp(end_time)\n",
    "\n",
    "        print(f\"\\n[{start_ts} - {end_ts}]\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        segments, info = model.transcribe(\n",
    "            chunk,\n",
    "            language=\"en\",\n",
    "            beam_size=5,\n",
    "            temperature=0.0,\n",
    "            vad_filter=True,\n",
    "            vad_parameters=dict(min_silence_duration_ms=500)\n",
    "        )\n",
    "\n",
    "        chunk_text = \"\"\n",
    "        for segment in segments:\n",
    "            chunk_text += segment.text + \" \"\n",
    "\n",
    "        chunk_text = chunk_text.strip()\n",
    "\n",
    "        # Print chunk transcription\n",
    "        print(chunk_text)\n",
    "\n",
    "        # Store\n",
    "        full_text.append(chunk_text)\n",
    "\n",
    "    print(\"\\n================= TRANSCRIPTION COMPLETE =================\\n\")\n",
    "\n",
    "    final_transcript = \" \".join(full_text)\n",
    "\n",
    "    print(\"\\n============= FULL TRANSCRIPT (COMBINED) =============\\n\")\n",
    "    print(final_transcript)\n",
    "\n",
    "    return full_text, final_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ae2f7",
   "metadata": {},
   "source": [
    "### Cell 9: Running Transcription on Podcast Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_texts, final_transcript = transcribe_long_audio_with_timestamps(\n",
    "    model,\n",
    "    \"clean_audio.wav\",\n",
    "    chunk_duration=45\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1604650b",
   "metadata": {},
   "source": [
    "### Cell 8: Saving Final Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff45985",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"/output/podcast_transcript_with_timestamps.txt\"\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, text in enumerate(chunk_texts):\n",
    "        start_time = idx * 45\n",
    "        end_time = start_time + len(text)\n",
    "\n",
    "        start_ts = format_timestamp(start_time)\n",
    "        end_ts = format_timestamp(start_time + 45)\n",
    "\n",
    "        f.write(f\"[{start_ts} - {end_ts}]\\n\")\n",
    "        f.write(text + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"\\n================ FULL TRANSCRIPT ================\\n\\n\")\n",
    "    f.write(final_transcript)\n",
    "\n",
    "print(\"✅ Transcript saved to:\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441de8d2",
   "metadata": {},
   "source": [
    "# Task 3: Research Topic Segmentation methods\n",
    "\n",
    "It accomplished basically researching about the the available methods for Topic Segmentation and following is a concise report for the same.\n",
    "\n",
    "Link: https://drive.google.com/file/d/1PVfo9WylX4lqOrTO2MT0cnDdks1DNGbn/view?usp=drive_link\n",
    "\n",
    "Finally selected to Segment topics using Transformer Based Deep Learning Approaches\n",
    "\n",
    "# Task 4: Topic Segmentation using Transformer-Based Deep Learning Approaches (alongwith validation)\n",
    "\n",
    "Report: https://drive.google.com/file/d/1DNxTWfeJeloGkiwoQOCv5WbXB9FCC7oo/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a543a",
   "metadata": {},
   "source": [
    "### Cell 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6608b3f8",
   "metadata": {},
   "source": [
    "!pip install transformers sentencepiece nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfbfb49",
   "metadata": {},
   "source": [
    "### Cell 2: Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19108a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da075ee6",
   "metadata": {},
   "source": [
    "### Cell 4: Load Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ce987",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSCRIPT_PATH = \"/podcast_transcription/podcast_transcript_with_timestamps.txt\"\n",
    "\n",
    "with open(TRANSCRIPT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    transcript = f.read()\n",
    "\n",
    "print(transcript[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b2bf8",
   "metadata": {},
   "source": [
    "### Cell 5: Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ad9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(transcript)\n",
    "print(f\"Total sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca6aebe",
   "metadata": {},
   "source": [
    "### Cell 6: Helper Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98926a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525dd771",
   "metadata": {},
   "source": [
    "## Approach 1: BERT Based segmentation\n",
    "\n",
    "**Logic:**\n",
    "\n",
    "We use **BERT Embeddings** -> Compute similarity -> Drop = Topic Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2639466",
   "metadata": {},
   "source": [
    "### Cell 1: Load BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24fb2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5535c9e6",
   "metadata": {},
   "source": [
    "### Cell 2: BERT Embedding Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed343650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text):\n",
    "    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5893510a",
   "metadata": {},
   "source": [
    "### Cell 3: Compute Similarities (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa7807",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings = [get_bert_embedding(s) for s in sentences]\n",
    "\n",
    "bert_similarities = []\n",
    "for i in range(len(bert_embeddings) - 1):\n",
    "    sim = cosine_similarity(bert_embeddings[i], bert_embeddings[i + 1])[0][0]\n",
    "    bert_similarities.append(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bfe582",
   "metadata": {},
   "source": [
    "### Cell 4: Detect Boundaries (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad78e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_threshold = 0.6\n",
    "bert_boundaries = [i for i, sim in enumerate(bert_similarities) if sim < bert_threshold]\n",
    "\n",
    "print(\"BERT topic boundaries at sentence indices:\", bert_boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2105a",
   "metadata": {},
   "source": [
    "### Cell 5: Build Segments (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3c379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_segments(sentences, boundaries):\n",
    "    segments = []\n",
    "    start = 0\n",
    "    for boundary in boundaries:\n",
    "        segment = \" \".join(sentences[start:boundary+1])\n",
    "        segments.append(segment)\n",
    "        start = boundary + 1\n",
    "    segments.append(\" \".join(sentences[start:]))\n",
    "    return segments\n",
    "\n",
    "bert_segments = build_segments(sentences, bert_boundaries)\n",
    "\n",
    "print(f\"Total BERT Segments: {len(bert_segments)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f49aa9",
   "metadata": {},
   "source": [
    "## Approach 2: GPT Based Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251966c3",
   "metadata": {},
   "source": [
    "### Cell 1: Load GPT Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e3dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dc7f39",
   "metadata": {},
   "source": [
    "### Cell 12: GPT Topic change detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd62329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_topic_change(sent1, sent2):\n",
    "    hypothesis = \"The topic of the two sentences is the same.\"\n",
    "    result = gpt_classifier(\n",
    "        sequences=sent1 + \" \" + sent2,\n",
    "        candidate_labels=[\"same topic\", \"different topic\"]\n",
    "    )\n",
    "    return result[\"labels\"][0] == \"different topic\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259f34a3",
   "metadata": {},
   "source": [
    "### Cell 3: Detect Boundaries (GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20047264",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_boundaries = []\n",
    "\n",
    "for i in range(len(sentences) - 1):\n",
    "    if gpt_topic_change(sentences[i], sentences[i + 1]):\n",
    "        gpt_boundaries.append(i)\n",
    "\n",
    "print(\"GPT topic boundaries:\", gpt_boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7ed4a0",
   "metadata": {},
   "source": [
    "### Cell 4: Build Segments (GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_segments = build_segments(sentences, gpt_boundaries)\n",
    "print(f\"Total GPT Segments: {len(gpt_segments)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a98bbd",
   "metadata": {},
   "source": [
    "## Approach 3: Longformer Based Segmentation\n",
    "\n",
    "**Longformer is built for long documents, and thus perfect for podcasts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf238d",
   "metadata": {},
   "source": [
    "### Cell 1: Load Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf13ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "long_model = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "long_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a35d5f",
   "metadata": {},
   "source": [
    "### Cell 2: Longformer Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d795b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longformer_embedding(text):\n",
    "    inputs = long_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "    with torch.no_grad():\n",
    "        outputs = long_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789880f6",
   "metadata": {},
   "source": [
    "### Cell 3: Compute Similarities (Longformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf432d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_embeddings = [get_longformer_embedding(s) for s in sentences]\n",
    "\n",
    "long_similarities = []\n",
    "for i in range(len(long_embeddings) - 1):\n",
    "    sim = cosine_similarity(long_embeddings[i], long_embeddings[i + 1])[0][0]\n",
    "    long_similarities.append(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e11bd",
   "metadata": {},
   "source": [
    "### Cell 4: Detect Boundaries (Longformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_threshold = 0.65\n",
    "long_boundaries = [i for i, sim in enumerate(long_similarities) if sim < long_threshold]\n",
    "\n",
    "print(\"Longformer topic boundaries:\", long_boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34e6ac",
   "metadata": {},
   "source": [
    "### Cell 5: Build Segments (Longformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_segments = build_segments(sentences, long_boundaries)\n",
    "print(f\"Total Longformer Segments: {len(long_segments)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea0584",
   "metadata": {},
   "source": [
    "## Comparison and Best model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32edfb22",
   "metadata": {},
   "source": [
    "### Cell 1: Compare Segment Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca1a1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BERT Segments:\", len(bert_segments))\n",
    "print(\"GPT Segments:\", len(gpt_segments))\n",
    "print(\"Longformer Segments:\", len(long_segments))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f083d292",
   "metadata": {},
   "source": [
    "### Cell 2: Print Sample Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09455fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== BERT Sample Segment ===\\n\", bert_segments[0][:500])\n",
    "print(\"\\n=== GPT Sample Segment ===\\n\", gpt_segments[0][:500])\n",
    "print(\"\\n=== Longformer Sample Segment ===\\n\", long_segments[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d2a553",
   "metadata": {},
   "source": [
    "### Cell 3: Simple Evaluation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc32ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"BERT\": len(bert_segments),\n",
    "    \"GPT\": len(gpt_segments),\n",
    "    \"Longformer\": len(long_segments)\n",
    "}\n",
    "\n",
    "best_model = min(results, key=lambda x: abs(results[x] - 14))  # 14 = expected segments\n",
    "print(\"Best performing model:\", best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836bbe27",
   "metadata": {},
   "source": [
    "## Saving the Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f30d6",
   "metadata": {},
   "source": [
    "### Cell 1: Saving the segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ef89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_segments_to_file(segments, model_name):\n",
    "    file_path = f\"/kaggle/working/{model_name.lower()}_all_segments.txt\"\n",
    "    \n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, segment in enumerate(segments):\n",
    "            f.write(f\"--- Segment {i+1} ---\\n\")\n",
    "            f.write(segment.strip() + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"✅ Saved ALL {len(segments)} segments to:\", file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75560790",
   "metadata": {},
   "source": [
    "### Cell 2: Running the function to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_all_segments_to_file(bert_segments, \"BERT\")\n",
    "save_all_segments_to_file(gpt_segments, \"GPT\")\n",
    "save_all_segments_to_file(long_segments, \"Longformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e185325d",
   "metadata": {},
   "source": [
    "## Approach 4: Topic Segmentation using LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5a072",
   "metadata": {},
   "source": [
    "### Cell 1: Initial setup + Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff5805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba110f55",
   "metadata": {},
   "source": [
    "### Cell 2: Setting up the LLM (gpt 4o, marketplace) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce57111",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://models.github.ai/inference\",\n",
    "    api_key=\"removed for privacy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2539e41f",
   "metadata": {},
   "source": [
    "### Cell 3: System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b80576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert system for topic segmentation of long podcast transcripts.\n",
    "\n",
    "Your task:\n",
    "- Segment the transcript into meaningful topical sections.\n",
    "- Detect natural discourse shifts (not paragraph breaks).\n",
    "- Assign a short, clear title to each segment.\n",
    "\n",
    "Rules:\n",
    "1. Each segment must have:\n",
    "   - A title (3–7 words)\n",
    "   - Start and end timestamps\n",
    "   - Original transcript text (no paraphrasing)\n",
    "2. Do NOT summarize.\n",
    "3. Do NOT remove or rewrite text.\n",
    "4. Preserve chronological order.\n",
    "5. Output format MUST be:\n",
    "\n",
    "Segment N:\n",
    "Title: <title>\n",
    "Time: <start timestamp> - <end timestamp>\n",
    "Text:\n",
    "<original transcript text>\n",
    "\n",
    "6. Ensure all content is covered.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2621328",
   "metadata": {},
   "source": [
    "### Cell 4: Loading the transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSCRIPT_PATH = \"/podcast_transcription/podcast_transcript_with_timestamps.txt\"\n",
    "\n",
    "with open(TRANSCRIPT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    full_transcript = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824c3c31",
   "metadata": {},
   "source": [
    "### Cell 5: Loading chunks in LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "MAX_TOKENS = 3500  # safe margin\n",
    "OVERLAP = 200\n",
    "\n",
    "def chunk_text(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + MAX_TOKENS\n",
    "        chunk = tokenizer.decode(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start = end - OVERLAP\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "transcript_chunks = chunk_text(full_transcript)\n",
    "print(\"Total LLM chunks:\", len(transcript_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabef28",
   "metadata": {},
   "source": [
    "### Cell 6: Getting LLM configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21919091",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_outputs = []\n",
    "\n",
    "for i, chunk in enumerate(transcript_chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(transcript_chunks)}\")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"openai/gpt-4o\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=4096,\n",
    "        top_p=1,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": chunk}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm_outputs.append(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47efb1f5",
   "metadata": {},
   "source": [
    "### Cell 7: Viewing LLM outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_llm_segmentation = \"\\n\\n\".join(llm_outputs)\n",
    "\n",
    "print(final_llm_segmentation[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd3b391",
   "metadata": {},
   "source": [
    "### Cell 8: Saving LLM Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c89300",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_OUTPUT_PATH = \"/output/llm_topic_segments.txt\"\n",
    "\n",
    "with open(LLM_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_llm_segmentation)\n",
    "\n",
    "print(\"✅ LLM topic segmentation saved to:\", LLM_OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
